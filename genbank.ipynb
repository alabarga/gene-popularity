{
 "metadata": {
  "name": "",
  "signature": "sha256:bad2aef70096a6560eb4b026627a833434ea545d7b42e7d238f2b04ccfab8e85"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "2+2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "4"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#sys.path.append('/Users/pkerp/Downloads/spark-1.1.1/bin')\n",
      "#from pyspark import SparkContext\n",
      "#from pyspark import SparkConf\n",
      "#conf = SparkConf()\n",
      "#conf.set(\"spark.shuffle.consolidateFiles\", \"true\")\n",
      " \n",
      "#sc = SparkContext()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def split_line_gene2pubmed(line):\n",
      "    parts = line.split()\n",
      "    return {\"tax_id\": int(parts[0]), \"GeneID\": int(parts[1]), \"PubMed_ID\": int(parts[2])}\n",
      "\n",
      "def split_line_gene_id(line):\n",
      "    parts = line.split('\\t')\n",
      "    return {'tax_id': int(parts[0]), \"GeneID\": int(parts[1]), \"Symbol\": parts[2],\n",
      "            'LocusTag': parts[3], 'Synonyms': parts[4], 'dbXrefs': parts[5],\n",
      "            'chromosome': parts[6], 'map_location': parts[7], 'description': parts[8],\n",
      "            'type_of_gene': parts[9], 'Symbol_from_nomenclature_authority': parts[10],\n",
      "            'Full_name_from_nomenclature_authority': parts[11], \n",
      "            'Nomenclature_stats': parts[12], 'Other_designations': parts[13],\n",
      "            'Modification_date': parts[14] }\n",
      "\n",
      "def filter_comments(line):\n",
      "    return line[0] != '#'\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gene2pubmed = sc.textFile('data/gene2pubmed').filter(filter_comments).map(split_line_gene2pubmed)\n",
      "#gene2pubmed = sc.textFile('data/short.csv').filter(filter_comments).map(split_line_gene2pubmed)\n",
      "gene_info = sc.textFile('data/gene_info').filter(filter_comments).map(split_line_gene_id)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#key here is GeneID\n",
      "id_pubmed_1 = gene2pubmed.map(lambda x: (x['GeneID'], 1)).cache()\n",
      "id_description_type = gene_info.map(lambda x: (x['GeneID'], (x['GeneID'], x['Symbol'], x['description'], x['type_of_gene'], x['tax_id']))).cache()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def count_func(v1, v2):\n",
      "    return v1+v2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# joining (geneid, pubmed\n",
      "id_description_type_pubmed = id_description_type.join(id_pubmed_1).map(lambda x: x[1][:3]).cache()\n",
      "id_description_type_pubmed.take(1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def rna_filter(x):\n",
      "    return 'rna' in x.lower()\n",
      "\n",
      "def ncrna_filter(x):\n",
      "    return 'ncrna' in x.lower()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sorted_citation_counts = id_description_type_pubmed.reduceByKey(count_func).map(lambda x: (x[1], x[0])).sortByKey(ascending=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print sorted_citation_counts.first()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(6622, (7157, u'TP53', u'tumor protein p53', u'protein-coding', 9606))\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rna_citation_counts = sorted_citation_counts.filter(lambda x: rna_filter(x[1][3]))\n",
      "ncrna_citation_counts = sorted_citation_counts.filter(lambda x: ncrna_filter(x[1][3]))\n",
      "\n",
      "take_all_citation_counts = sorted_citation_counts.take(100000)\n",
      "take_rna_citation_counts = rna_citation_counts.take(100000)\n",
      "take_ncrna_citation_counts = ncrna_citation_counts.take(100000)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for trc in take_rna_citation_counts[:800000]:\n",
      "    if trc[1][-1] == 9606:\n",
      "        # look only at human entries\n",
      "        if 'rrna' in str(trc).lower():\n",
      "            print str(trc)\n",
      "        #print \"\\n\".join(map(str, take_rna_citation_counts[:40]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(74, (4549, u'RNR1', u's-rRNA', u'rRNA', 9606))\n",
        "(15, (4550, u'RNR2', u'l-rRNA', u'rRNA', 9606))\n",
        "(8, (100008589, u'RNA28S5', u'RNA, 28S ribosomal 5', u'rRNA', 9606))\n",
        "(6, (100008588, u'RNA18S5', u'RNA, 18S ribosomal 5', u'rRNA', 9606))\n",
        "(2, (100008587, u'RNA5-8S5', u'RNA, 5.8S ribosomal 5', u'rRNA', 9606))\n",
        "(1, (100861532, u'RNA45S5', u'RNA, 45S pre-ribosomal 5', u'rRNA', 9606))\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def to_html_table(items, headings=None, links=None, link_column=None):\n",
      "    '''\n",
      "    Create an html table from a 2D list\n",
      "    '''\n",
      "    out_str = \"<table>\\n\"\n",
      "    if headings is not None:\n",
      "        out_str += \"\\t<tr>\\n\"\n",
      "        for h in headings:\n",
      "            out_str += '\\t\\t<th>{}</th>\\n'.format(h)\n",
      "        out_str += \"\\t</tr>\\n\"\n",
      "    \n",
      "    for it, item in enumerate(items):\n",
      "        out_str += \"\\t<tr>\\n\"\n",
      "        for j,i in enumerate(item):\n",
      "            this_entry = \"{}\".format(i)\n",
      "            if links is not None and j in links.keys():\n",
      "                this_entry = '<a href=\"{}\">{}</a>'.format(links[j][it], this_entry)\n",
      "            out_str += \"\\t\\t<td>{}</td>\\n\".format(this_entry)\n",
      "        out_str += \"\\t</tr>\\n\"\n",
      "    \n",
      "    out_str += \"</table>\"\n",
      "    return out_str"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import json\n",
      "\n",
      "species_lookup = {9606: 'Homo sapiens', 10090: 'Mus musculus', 7227: \"Drosophila melanogaster\", 11676: \"Human immunodeficiency virus 1\"}\n",
      "\n",
      "def citation_counts_to_json(citation_counts):\n",
      "    counts = []\n",
      "    for cc in citation_counts:\n",
      "        counts += [{\"name\": cc[1][1], \"description\": cc[1][2], \"count\":cc[0], 'species':species_lookup[cc[1][4]]}]\n",
      "    \n",
      "    print json.dumps(counts)\n",
      "\n",
      "def citation_counts_to_csv(citation_counts):\n",
      "    print \"name\\tdescription\\tcount\"\n",
      "    for cc in citation_counts:\n",
      "        print \"\\t\".join(map(str, [cc[1][1], cc[1][2], cc[0]]))\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i, (count, (gid, symbol, description, gene_type, tax_id)) in enumerate(take_all_citation_counts[:1000]):\n",
      "    if \"actin\" in description.lower() or \"myosin\" in description.lower():\n",
      "        print i, count, symbol, description"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "163 831 Acta2 actin, alpha 2, smooth muscle, aorta\n",
        "547 433 Act5C Actin 5C\n",
        "589 409 Mhc Myosin heavy chain\n",
        "590 408 ACTB actin, beta\n",
        "636 392 myl7 myosin, light chain 7, regulatory\n",
        "823 333 ACTA1 actin, alpha 1, skeletal muscle\n",
        "884 317 SMARCA4 SWI/SNF related, matrix associated, actin dependent regulator of chromatin, subfamily a, member 4\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### The data used to create the Most Popular Genes posting ###"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "citation_counts_to_json(take_all_citation_counts[:20])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[{\"count\": 6622, \"species\": \"Homo sapiens\", \"name\": \"TP53\", \"description\": \"tumor protein p53\"}, {\"count\": 4436, \"species\": \"Homo sapiens\", \"name\": \"TNF\", \"description\": \"tumor necrosis factor\"}, {\"count\": 4350, \"species\": \"Mus musculus\", \"name\": \"Gt(ROSA)26Sor\", \"description\": \"gene trap ROSA 26, Philippe Soriano\"}, {\"count\": 3503, \"species\": \"Drosophila melanogaster\", \"name\": \"w\", \"description\": \"white\"}, {\"count\": 3445, \"species\": \"Homo sapiens\", \"name\": \"UBC\", \"description\": \"ubiquitin C\"}, {\"count\": 3422, \"species\": \"Homo sapiens\", \"name\": \"APOE\", \"description\": \"apolipoprotein E\"}, {\"count\": 3410, \"species\": \"Homo sapiens\", \"name\": \"EGFR\", \"description\": \"epidermal growth factor receptor\"}, {\"count\": 3273, \"species\": \"Mus musculus\", \"name\": \"Trp53\", \"description\": \"transformation related protein 53\"}, {\"count\": 3146, \"species\": \"Homo sapiens\", \"name\": \"VEGFA\", \"description\": \"vascular endothelial growth factor A\"}, {\"count\": 3120, \"species\": \"Homo sapiens\", \"name\": \"IL6\", \"description\": \"interleukin 6\"}, {\"count\": 2863, \"species\": \"Homo sapiens\", \"name\": \"TGFB1\", \"description\": \"transforming growth factor, beta 1\"}, {\"count\": 2784, \"species\": \"Homo sapiens\", \"name\": \"MTHFR\", \"description\": \"methylenetetrahydrofolate reductase (NAD(P)H)\"}, {\"count\": 2670, \"species\": \"Human immunodeficiency virus 1\", \"name\": \"env\", \"description\": \"gp160; envelope glycoprotein\"}, {\"count\": 2357, \"species\": \"Homo sapiens\", \"name\": \"ESR1\", \"description\": \"estrogen receptor 1\"}, {\"count\": 2340, \"species\": \"Homo sapiens\", \"name\": \"HLA-DRB1\", \"description\": \"major histocompatibility complex, class II, DR beta 1\"}, {\"count\": 2256, \"species\": \"Mus musculus\", \"name\": \"Tnf\", \"description\": \"tumor necrosis factor\"}, {\"count\": 2244, \"species\": \"Homo sapiens\", \"name\": \"NFKB1\", \"description\": \"nuclear factor of kappa light polypeptide gene enhancer in B-cells 1\"}, {\"count\": 2202, \"species\": \"Homo sapiens\", \"name\": \"IL10\", \"description\": \"interleukin 10\"}, {\"count\": 2137, \"species\": \"Homo sapiens\", \"name\": \"ACE\", \"description\": \"angiotensin I converting enzyme\"}, {\"count\": 2087, \"species\": \"Homo sapiens\", \"name\": \"BRCA1\", \"description\": \"breast cancer 1, early onset\"}]\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import random\n",
      "\n",
      "# create a list of random numbers with a 'count' of 1\n",
      "a = sc.parallelize([(random.randint(0,10),1) for i in range(20)])\n",
      "\n",
      "# create a list of doubled numbers\n",
      "b = sc.parallelize([(i, i*2) for i in range(20)])\n",
      "ab = a.join(b)\n",
      "\n",
      "print len(ab.collect()), ab.collect()\n",
      "\n",
      "# reduce function\n",
      "def count(v1,v2):\n",
      "    return v1 + v2\n",
      "\n",
      "# count the occurances of the random numbers and order decreasing\n",
      "c = a.reduceByKey(count).map(lambda x: (-x[1], x[0])).sortByKey()\n",
      "d = c.map(lambda x: (x[1], -x[0]))\n",
      "\n",
      "print d.collect()\n",
      "print d.join(b).collect()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "20 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(8, (1, 16)), (8, (1, 16)), (8, (1, 16)), (8, (1, 16)), (9, (1, 18)), (9, (1, 18)), (1, (1, 2)), (2, (1, 4)), (2, (1, 4)), (2, (1, 4)), (2, (1, 4)), (10, (1, 20)), (10, (1, 20)), (10, (1, 20)), (3, (1, 6)), (4, (1, 8)), (4, (1, 8)), (6, (1, 12)), (7, (1, 14)), (7, (1, 14))]\n",
        "[(8, 4), (2, 4), (10, 3), (4, 2), (9, 2), (7, 2), (1, 1), (6, 1), (3, 1)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[(8, (4, 16)), (9, (2, 18)), (1, (1, 2)), (2, (4, 4)), (10, (3, 20)), (3, (1, 6)), (4, (2, 8)), (6, (1, 12)), (7, (2, 14))]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Most Popular Genes By Year ###\n",
      "\n",
      "The plan is to rank genes according to how popular they are in each year.\n",
      "The simples way to do that might to be have a loop and then filter\n",
      "the data by the year in question and produce a top twenty list for that\n",
      "year.\n",
      "\n",
      "1. Load the year-pmids table\n",
      "2. filter for the year in question\n",
      "3. filter the gene-pmids table by pmids that are in the year-pmids table\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import datetime as dt\n",
      "\n",
      "def parse_date_pmid_line(line):\n",
      "    # extract the date information and the pmid\n",
      "    # and return it as tuple\n",
      "    parts = line.split()\n",
      "    date = dt.datetime.strptime(parts[0], '%Y/%m/%d')\n",
      "    pmid = int(parts[1])\n",
      "    return (pmid, date.year)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# load the dates, now in the form of tuples: (year, pmid)\n",
      "pmid_year = sc.textFile('data/pmid_by_date/*.ssv').map(parse_date_pmid_line)\n",
      "\n",
      "#this needs to be joined to the gene_ids table such that we have a list\n",
      "#with key, value pairs equal to ((gene_id, year)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "id_description_type_pubmed.map(lambda x: x[0]).take(1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 22,
       "text": [
        "[(18874369,\n",
        "  u'SPAPADRAFT_62546',\n",
        "  u'U1 snRNP protein',\n",
        "  u'protein-coding',\n",
        "  619300)]"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pmid_id_desc_type = id_description_type_pubmed.map(lambda x: (x[0][-1], x[0][:-1]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pmid_id_desc_type.take(1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 24,
       "text": [
        "[(619300,\n",
        "  (18874369, u'SPAPADRAFT_62546', u'U1 snRNP protein', u'protein-coding'))]"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pmid_year.take(1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 25,
       "text": [
        "[(9414895, 1998)]"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pmid_id_desc_type_year = pmid_id_desc_type.join(pmid_year)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pmid_id_desc_type_year.take(1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "Py4JJavaError",
       "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 62.0 failed 1 times, most recent failure: Lost task 0.0 in stage 62.0 (TID 535, localhost): java.io.FileNotFoundException: /var/folders/gw/xv978cnj20j909xvdlxq3jb00000gn/T/spark-local-20141216180627-112e/29/shuffle_7_0_147 (Too many open files in system)\n        java.io.FileOutputStream.openAppend(Native Method)\n        java.io.FileOutputStream.<init>(FileOutputStream.java:192)\n        org.apache.spark.storage.DiskBlockObjectWriter.open(BlockObjectWriter.scala:123)\n        org.apache.spark.storage.DiskBlockObjectWriter.write(BlockObjectWriter.scala:192)\n        org.apache.spark.shuffle.hash.HashShuffleWriter$$anonfun$write$1.apply(HashShuffleWriter.scala:67)\n        org.apache.spark.shuffle.hash.HashShuffleWriter$$anonfun$write$1.apply(HashShuffleWriter.scala:65)\n        scala.collection.Iterator$class.foreach(Iterator.scala:727)\n        scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n        org.apache.spark.shuffle.hash.HashShuffleWriter.write(HashShuffleWriter.scala:65)\n        org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)\n        org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n        org.apache.spark.scheduler.Task.run(Task.scala:54)\n        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)\n        java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)\n        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)\n        java.lang.Thread.run(Thread.java:695)\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1185)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1174)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1173)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1173)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:688)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:688)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:688)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1391)\n\tat akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)\n\tat akka.actor.ActorCell.invoke(ActorCell.scala:456)\n\tat akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)\n\tat akka.dispatch.Mailbox.run(Mailbox.scala:219)\n\tat akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n\tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n\tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n\tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-27-6d7dab0ffc9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpmid_id_desc_type_year\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m/Users/pkerp/Downloads/spark-1.1.1/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1150\u001b[0m             p = range(\n\u001b[1;32m   1151\u001b[0m                 partsScanned, min(partsScanned + numPartsToTry, totalParts))\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/pkerp/Downloads/spark-1.1.1/python/pyspark/context.pyc\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 770\u001b[0;31m         \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjavaPartitions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallowLocal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    771\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_collect_iterator_through_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/pkerp/Downloads/spark-1.1.1/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[0;32m--> 538\u001b[0;31m                 self.target_id, self.name)\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/pkerp/Downloads/spark-1.1.1/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    299\u001b[0m                     \u001b[0;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[1;32m    301\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 raise Py4JError(\n",
        "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 62.0 failed 1 times, most recent failure: Lost task 0.0 in stage 62.0 (TID 535, localhost): java.io.FileNotFoundException: /var/folders/gw/xv978cnj20j909xvdlxq3jb00000gn/T/spark-local-20141216180627-112e/29/shuffle_7_0_147 (Too many open files in system)\n        java.io.FileOutputStream.openAppend(Native Method)\n        java.io.FileOutputStream.<init>(FileOutputStream.java:192)\n        org.apache.spark.storage.DiskBlockObjectWriter.open(BlockObjectWriter.scala:123)\n        org.apache.spark.storage.DiskBlockObjectWriter.write(BlockObjectWriter.scala:192)\n        org.apache.spark.shuffle.hash.HashShuffleWriter$$anonfun$write$1.apply(HashShuffleWriter.scala:67)\n        org.apache.spark.shuffle.hash.HashShuffleWriter$$anonfun$write$1.apply(HashShuffleWriter.scala:65)\n        scala.collection.Iterator$class.foreach(Iterator.scala:727)\n        scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n        org.apache.spark.shuffle.hash.HashShuffleWriter.write(HashShuffleWriter.scala:65)\n        org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)\n        org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n        org.apache.spark.scheduler.Task.run(Task.scala:54)\n        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)\n        java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)\n        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)\n        java.lang.Thread.run(Thread.java:695)\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1185)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1174)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1173)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1173)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:688)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:688)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:688)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1391)\n\tat akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)\n\tat akka.actor.ActorCell.invoke(ActorCell.scala:456)\n\tat akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)\n\tat akka.dispatch.Mailbox.run(Mailbox.scala:219)\n\tat akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n\tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n\tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n\tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}